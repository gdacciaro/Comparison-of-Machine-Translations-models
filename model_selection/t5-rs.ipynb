{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install sacrebleu sentencepiece &> /dev/null\n",
    "!pip install nltk==3.2.4\n",
    "!pip install transformers[sentencepiece] datasets &> /dev/null\n",
    "!pip install sacrebleu sentencepiece &> /dev/null\n",
    "!pip install huggingface_hub &> /dev/null\n",
    "!pip install datasets transformers[sentencepiece] sacrebleu &> /dev/null\n",
    "!pip install telepot &> /dev/null\n",
    "!pip install evaluate"
   ],
   "metadata": {
    "id": "bxsP7_JK4MGy",
    "outputId": "0a868d95-06c1-4273-aaf6-603b145b1f24",
    "execution": {
     "iopub.status.busy": "2022-07-18T21:51:03.881740Z",
     "iopub.execute_input": "2022-07-18T21:51:03.882135Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!rm ita-eng.zip &> /dev/null\n",
    "!rm rm -rf dataset &> /dev/null\n",
    "!wget \"https://www.manythings.org/anki/ita-eng.zip\" &> /dev/null\n",
    "!unzip \"ita-eng.zip\" -d \"dataset\" &> /dev/null\n",
    "\n",
    "text_file = \"dataset/ita.txt\""
   ],
   "metadata": {
    "id": "3VaDX9Rn4MG2",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import platform\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import AdamWeightDecay\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##############################################################################\n",
    "RANDOM_SEARCH = True # if false, the model will be trained using the following hyperparameters:\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-05\n",
    "WEIGHT_DECAY = 1e-06\n",
    "EPOCHS_ = 1\n",
    "##############################################################################\n",
    "token = \"5597879510:AAH1FSuZa7lA_xoAp-7JiUDE0OY38p-Tq5M\"\n",
    "theUrl = \"https://api.telegram.org/bot\" + token + \"/sendMessage\"\n",
    "who = \"-606080513\"\n",
    "\n",
    "random_search_trial = 3 # Not used if RANDOM_SEARCH is False\n",
    "epochs = 5              # Not used if RANDOM_SEARCH is False\n",
    "\n",
    "num_of_test = 30  # Number of sentences in the test set used to calculate the metric\n",
    "text_file = \"dataset/ita.txt\"\n",
    "prefix = \"translate English to Italian: \"\n",
    "file_name = 'full_t5_result.csv'\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"it\"\n",
    "##############################################################################\n",
    "\n",
    "pc_name = platform.node()\n",
    "\n",
    "def sendMessage(text=\"\"):\n",
    "    import json\n",
    "    import requests\n",
    "    data = {'chat_id': who, 'disable_notification': 'false', 'text': text}\n",
    "    headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "    #requests.post(theUrl, data=json.dumps(data, ensure_ascii=True), headers=headers)\n",
    "\n",
    "def sendFile(file):\n",
    "    import telepot\n",
    "    f = open(file, 'rb')\n",
    "    bot = telepot.Bot(token)\n",
    "    bot.sendDocument(who, f)\n",
    "    \n",
    "if(RANDOM_SEARCH):\n",
    "    true_epochs = epochs\n",
    "else:\n",
    "    true_epochs = EPOCHS_\n",
    "\n",
    "text = \"\"\"\n",
    ">>> Starting T5... [\"\"\"+pc_name+\"\"\"]\n",
    "Params:\n",
    "is a random search?: \"\"\"+str(RANDOM_SEARCH)+\"\"\"\n",
    "epochs: \"\"\"+str(true_epochs)+\"\"\"\n",
    "num_of_test: \"\"\"+str(num_of_test)+\"\"\"\n",
    "random_search_trial: \"\"\"+str(random_search_trial)+\"\"\"\n",
    "\"\"\"\n",
    "sendMessage(text)\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, ita, _copyright = line.split(\"\\t\")\n",
    "    text_pairs.append({\"translation\": {\"en\": eng, \"it\": ita}})\n",
    "\n",
    "# Now, let's split the sentence pairs into a training set, a validation set, and a test set.\n",
    "random.seed(42)\n",
    "#random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
    "\n",
    "print(f\"{len(text_pairs)} total sentences\\n---\\n\")\n",
    "print(f\"{len(train_pairs)} training sentences\")\n",
    "print(f\"{len(val_pairs)} validation sentences\")\n",
    "print(f\"{len(test_pairs)} test sentences\\n---\\n\")\n",
    "\n",
    "raw_datasets = datasets.DatasetDict({\"train\": datasets.Dataset.from_pandas(pd.DataFrame(data=train_pairs)),\n",
    "                                     \"validation\": datasets.Dataset.from_pandas(pd.DataFrame(data=val_pairs)),\n",
    "                                     \"test\": datasets.Dataset.from_pandas(pd.DataFrame(data=test_pairs))})\n",
    "\n",
    "model_checkpoint = \"t5-small\"  ## \"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"\n",
    "model_name = model_checkpoint\n",
    "print(model_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "def decode_sequence(model, sentence):\n",
    "    inputs = tokenizer(prefix + sentence, return_tensors=\"tf\").input_ids\n",
    "    outputs = model.generate(inputs)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "def metric(target,output):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    reference = [target.split(\" \")]\n",
    "    candidate = output.split(\" \")\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    return score\n",
    "\n",
    "def calculate_metric_on_testset(modello):\n",
    "    result = 0\n",
    "    count = 0\n",
    "    for item in test_pairs[:num_of_test]:\n",
    "        input_eng = item[\"translation\"][\"en\"]  # eng\n",
    "        target = item[\"translation\"][\"it\"]   # ita\n",
    "        output = decode_sequence(modello, input_eng)\n",
    "        item_result = metric(target, output)\n",
    "        result += item_result\n",
    "        count += 1\n",
    "\n",
    "    final_result = result / count\n",
    "    return final_result\n",
    "\n",
    "def append_to_csv(lista):\n",
    "    with open(file_name, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(lista)\n",
    "\n",
    "def one_single_trial(model, batch_size, epochs, learning_rate, weight_decay):\n",
    "    error = False\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"tf\")\n",
    "    train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "        batch_size=batch_size,\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator, )\n",
    "\n",
    "    validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "        batch_size=batch_size,\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator, )\n",
    "\n",
    "    optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "    model.compile(optimizer=optimizer)  # To try: , loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs)\n",
    "        end_time = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        metric_result = calculate_metric_on_testset(model)\n",
    "        metric_end_time = time.time() - start_time\n",
    "    except:\n",
    "        end_time = -1\n",
    "        metric_end_time = -1\n",
    "        metric_result = -1\n",
    "        error = True\n",
    "\n",
    "    lista = [str(batch_size), str(learning_rate), str(weight_decay), str(epochs), str(end_time),\n",
    "             str(metric_end_time), str(metric_result), str(error)]\n",
    "    append_to_csv(lista)\n",
    "    metric_end_time += end_time\n",
    "    return model, metric_result, lista, end_time\n",
    "\n",
    "def train():\n",
    "    global random_search_trial\n",
    "    global epochs\n",
    "\n",
    "    total_time = 0\n",
    "    model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "    print(model.summary())\n",
    "    append_to_csv(['batch_size', 'learning_rate', 'weight_decay', 'epochs', 'training_time', 'metric_time', 'metric', 'error'])\n",
    "    models = []\n",
    "\n",
    "    if not RANDOM_SEARCH:\n",
    "        random_search_trial = 1\n",
    "\n",
    "    for _ in range(random_search_trial):\n",
    "        if RANDOM_SEARCH:\n",
    "            batch_size = random.choice([16,32,64])\n",
    "            learning_rate = random.choice([0.00001, 0.000001, 1e-06, 1e-07])\n",
    "            weight_decay = random.choice([0.00001, 0.000001, 1e-06, 1e-07])\n",
    "        else:\n",
    "            batch_size = BATCH_SIZE\n",
    "            learning_rate = LEARNING_RATE\n",
    "            weight_decay = WEIGHT_DECAY\n",
    "            epochs = EPOCHS_\n",
    "\n",
    "        print(\"Parameter selected: batch_size=\", batch_size,\n",
    "              \", learning_rate=\", learning_rate,\n",
    "              \", weight_decay=\", weight_decay)\n",
    "\n",
    "        model, metric_result, lista, end_time = one_single_trial(model, batch_size, epochs, learning_rate, weight_decay)\n",
    "        models.append((model, metric_result, lista))\n",
    "\n",
    "    model, best_result, hyperparams = models[np.argmax([y for x, y, z in models])]\n",
    "\n",
    "    test_sentence = \"So long and thanks for all the fish\"\n",
    "    translated_test_sentence = decode_sequence(model, test_sentence)\n",
    "\n",
    "    test_sentence_2 = \"Space is big. You just won't believe how vastly, hugely, mind-bogglingly big it is.\"\n",
    "    translated_test_sentence_2 = decode_sequence(model, test_sentence_2)\n",
    "\n",
    "    test_sentence_3 = test_pairs[71][\"translation\"][\"en\"]\n",
    "    translated_test_sentence_3 = decode_sequence(model, test_sentence_3)\n",
    "\n",
    "    text = \"\"\"\n",
    "💻 Worker: \"\"\"+pc_name+\"\"\"\n",
    "✨ Finished training ✨\n",
    "🧠 Model: \"\"\"+model_checkpoint+\"\"\"\n",
    "🕒 Total time: \"\"\" + str(round(total_time, 6)) + \"\"\"\n",
    "📈 Best result: \"\"\" + str(round(best_result, 6)) + \"\"\"\n",
    "🔍 Params \"\"\" + str(hyperparams) + \"\"\"\n",
    "\\n\n",
    "Test sentence 1:\n",
    "🇬🇧 : \"\"\" + test_sentence + \"\"\"\n",
    "🇮🇹 : \"\"\" + translated_test_sentence + \"\"\"\n",
    "\\n\n",
    "Test sentence 2:\n",
    "🇬🇧 : \"\"\" + test_sentence_2 + \"\"\"\n",
    "🇮🇹 : \"\"\" + translated_test_sentence_2 + \"\"\"\\n\n",
    "Test sentence 3:\n",
    "🇬🇧 : \"\"\" + test_sentence_3 + \"\"\"\n",
    "🇮🇹 : \"\"\" + translated_test_sentence_3 + \"\"\"\n",
    "\"\"\"\n",
    "    sendMessage(text)\n",
    "    sendFile(file_name)\n",
    "    return model\n",
    "\n",
    "model = train()"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "id": "YCRZjuJ74MG3",
    "outputId": "2fb175c4-71d9-4274-b46c-254f4ee76875",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_metric_on_testset(modello):\n",
    "    result = 0\n",
    "    count = 0\n",
    "    for item in test_pairs[:num_of_test]:\n",
    "        input_eng = item[\"translation\"][\"en\"]  # eng\n",
    "        target = item[\"translation\"][\"it\"]   # ita\n",
    "        output = decode_sequence(modello, input_eng)\n",
    "        item_result = metric(target, output)\n",
    "        result += item_result\n",
    "        count += 1\n",
    "\n",
    "    final_result = result / count\n",
    "    return final_result"
   ],
   "metadata": {
    "id": "clBC0LfsKUCJ",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "calculate_metric_on_testset(model)"
   ],
   "metadata": {
    "id": "0VrFKLFHJ2kC",
    "outputId": "5ea740d9-2f44-4533-f214-7500823a1119",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Export\n",
    "import tensorflow as tf\n",
    "import os\n",
    "model.save_weights('save/')\n",
    "\n",
    "!zip -r method2_t5.zip save\n",
    "\n",
    "!split -b 45m method2_t5.zip method2_t5.zip.pt_\n",
    "\n",
    "for file in os.listdir(os.getcwd()):\n",
    "        if file.startswith(\"method2_t5.zip.pt_\"):\n",
    "            sendFile(file)"
   ],
   "metadata": {
    "id": "ovJ1oEr44MG7",
    "outputId": "9d6cc2cf-dbb3-4df4-de81-0ff1aa9a7d9a",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}