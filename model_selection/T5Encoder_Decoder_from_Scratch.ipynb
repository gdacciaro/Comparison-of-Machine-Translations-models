{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VKw43aViMSTJ"
      },
      "outputs": [],
      "source": [
        "! pip install transformers[sentencepiece] datasets &> /dev/null\n",
        "! pip install sacrebleu sentencepiece &> /dev/null\n",
        "! pip install huggingface_hub &> /dev/null\n",
        "! pip install datasets transformers[sentencepiece] sacrebleu &> /dev/null\n",
        "! pip install telepot &> /dev/null\n",
        "!pip install nltk==3.2.4 &> /dev/null\n",
        "!pip install -q -U tensorflow-addons &> /dev/null\n",
        "!pip install evaluate &> /dev/null\n",
        "!pip install keras_tuner &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EEmEoROaM28x"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "import pathlib\n",
        "import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import T5TokenizerFast, BertTokenizer\n",
        "from tensorflow import keras\n",
        "from transformers import BertTokenizerFast\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import AdamWeightDecay\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from transformers import AutoTokenizer,GPT2Tokenizer\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from transformers import TFT5EncoderModel, T5Tokenizer\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMUm7AZSk2fN",
        "outputId": "98399074-c1aa-4147-850e-774f5545018a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.41.177.210:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.41.177.210:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.41.177.210:8470']\n",
            "Number of accelerators:  8\n"
          ]
        }
      ],
      "source": [
        "# Detect hardware\n",
        "try:\n",
        "    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "except ValueError:\n",
        "    tpu_resolver = None\n",
        "    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "\n",
        "# Select appropriate distribution strategy\n",
        "if tpu_resolver:\n",
        "    tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu_resolver)\n",
        "    print('Running on TPU ', tpu_resolver.cluster_spec().as_dict()['worker'])\n",
        "elif len(gpus) > 1:\n",
        "    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
        "    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
        "elif len(gpus) == 1:\n",
        "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "    print('Running on single GPU ', gpus[0].name)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "    print('Running on CPU')\n",
        "  \n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs2zDzIrZc8P"
      },
      "source": [
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vXbusS2fNXNR"
      },
      "outputs": [],
      "source": [
        "!rm ita-eng.zip &> /dev/null\n",
        "!rm rm -rf dataset &> /dev/null\n",
        "!wget \"https://www.manythings.org/anki/ita-eng.zip\" &> /dev/null\n",
        "!unzip \"ita-eng.zip\" -d \"dataset\" &> /dev/null\n",
        "\n",
        "text_file = \"dataset/ita.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CZN9J-E6Thni"
      },
      "outputs": [],
      "source": [
        "sequence_length = 90\n",
        "\n",
        "\n",
        "def split_set(dataset: tf.data.Dataset,\n",
        "              tr: float = 0.8,\n",
        "              val: float = 0.1,\n",
        "              ts: float = 0.1,\n",
        "              shuffle: bool = True) -> (tf.data.Dataset, tf.data.Dataset, tf.data.Dataset):\n",
        "    if tr+val+ts != 1:\n",
        "        raise ValueError(\"Train, validation and test partition not allowed with such splits\")\n",
        "\n",
        "    dataset_size = dataset.cardinality().numpy()\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(dataset_size)\n",
        "\n",
        "    tr_size = int(tr * dataset_size)\n",
        "    val_size = int(val * dataset_size)\n",
        "\n",
        "    tr_set = dataset.take(tr_size)\n",
        "    val_set = dataset.skip(tr_size).take(val_size)\n",
        "    ts_set = dataset.skip(tr_size).skip(val_size)\n",
        "    return tr_set, val_set, ts_set\n",
        "\n",
        "\n",
        "def make_batches(dataset_src_dst: tf.data.Dataset, batch_size: int):\n",
        "    return dataset_src_dst.cache().batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "def create_dataset(name: str, preprocessed:bool):\n",
        "    with open(name, encoding=\"UTF-8\") as datafile:\n",
        "        src_set = list()\n",
        "        dst_set = list()\n",
        "        for sentence in datafile:\n",
        "            sentence = sentence.split(\"\\t\")\n",
        "            src_set.append(sentence[0])\n",
        "            if preprocessed:\n",
        "                dst_set.append(sentence[1].split(\"\\n\")[0])\n",
        "            else:\n",
        "                dst_set.append(sentence[1])\n",
        "\n",
        "    return src_set, dst_set\n",
        "\n",
        "\n",
        "\n",
        "def format_dataset(src, trg):\n",
        "    return ({\"encoder_inputs\": src, \"decoder_inputs\": trg[:, :-1]}, trg[:, 1:])\n",
        "\n",
        "def make_dataset(dataset, batch_size):\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.prefetch(tf.data.experimental.AUTOTUNE).cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezb3W8n_Z6zK"
      },
      "source": [
        "# **Tokenizer**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5jtJmrlGZ5nI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "source_src= \"google/t5-v1_1-small\"\n",
        "target_src = \"dbmdz/bert-base-italian-cased\" \n",
        "\n",
        "tokenizer_src = T5Tokenizer.from_pretrained(source_src)\n",
        "tokenizer_trg = BertTokenizerFast.from_pretrained(target_src)\n",
        "\n",
        "v_size_src = tokenizer_src.vocab_size\n",
        "v_size_trg = tokenizer_trg.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NG7FcKqQaIWX"
      },
      "outputs": [],
      "source": [
        "def tokenize(entire_set):\n",
        "\n",
        "  source_set, target_set = entire_set\n",
        "  tokens_source = tokenizer_src(source_set, truncation=True, padding=\"max_length\",\n",
        "                              return_tensors=\"tf\", max_length=sequence_length).data[\"input_ids\"]\n",
        "  tokens_source = tf.cast(tokens_source, dtype=tf.int32)                            \n",
        "  tokens_target = tokenizer_trg(target_set, add_special_tokens=True,\n",
        "                              truncation=True, padding=\"max_length\",\n",
        "                              return_tensors=\"tf\", max_length=sequence_length + 1).data[\"input_ids\"]\n",
        "  tokens_target = tf.cast(tokens_target, dtype=tf.int32)\n",
        "  return tokens_source, tokens_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5sGZMVu_adgP"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(tokenize(create_dataset(\"dataset/ita.txt\", False)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "I6e-b7ooaf_H"
      },
      "outputs": [],
      "source": [
        "tr_set, val_set, ts_set = split_set(dataset, 0.9, 0.05, 0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-eYW6Zyaumh"
      },
      "source": [
        "# **Encoder-Decoder Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9Y149xgnaipz"
      },
      "outputs": [],
      "source": [
        "dropout_rate = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYlUmtn2ais1",
        "outputId": "99a6ec85-be90-4a6f-df21-934f63c4ee6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/t5-v1_1-small were not used when initializing TFT5EncoderModel: ['lm_head', 'decoder']\n",
            "- This IS expected if you are initializing TFT5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFT5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFT5EncoderModel were initialized from the model checkpoint at google/t5-v1_1-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5EncoderModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "  encoder = TFT5EncoderModel.from_pretrained(source_src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "blHaHBr-RGec"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"elu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:  \n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        else:\n",
        "            assert False\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "      super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "      self.token_embeddings = layers.Embedding(\n",
        "          input_dim= vocab_size, output_dim=embed_dim\n",
        "      )\n",
        "      self.position_embeddings = layers.Embedding(\n",
        "          input_dim=sequence_length, output_dim=embed_dim\n",
        "      )\n",
        "      self.sequence_length = sequence_length\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embed_dim = embed_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "      length = tf.shape(inputs)[-1] \n",
        "      positions = tf.range(start=0, limit=length, delta=1)\n",
        "      embedded_tokens = self.token_embeddings(inputs)\n",
        "      embedded_positions = self.position_embeddings(positions)\n",
        "      return embedded_tokens + embedded_positions\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "      return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"elu\"), layers.Dropout(dropout_rate), layers.Dense(embed_dim)])\n",
        "        #self.dense_proj_f = keras.Sequential(\n",
        "        #    [layers.Dense(latent_dim, activation=\"elu\"), layers.Dropout(dropout_rate), layers.Dense(embed_dim)])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        #self.layernorm_1_f = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "        #self.dropout_1 = layers.Dropout(dropout_rate)\n",
        "        #self.dropout_2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "        self.resid1=layers.Add()\n",
        "        self.resid2=layers.Add()\n",
        "        self.resid3=layers.Add()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        \n",
        "        #attention_output_1 = self.dropout_1(attention_output_1)\n",
        "        out_1 = self.layernorm_1(self.resid1([inputs, attention_output_1]))\n",
        "        #proj_output_f = self.dense_proj_f(out_1)\n",
        "        #out_1 = self.layernorm_1_f(layers.Add()([out_1, proj_output_f]))\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask\n",
        "        )\n",
        "        #attention_output_2 = self.dropout_2(attention_output_2)\n",
        "        out_2 = self.layernorm_2(self.resid2([out_1, attention_output_2]))\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        #proj_output = self.dropout_1(proj_output)\n",
        "        return self.layernorm_3(self.resid3([out_2, proj_output]))\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oPcd0fSvRWtQ"
      },
      "outputs": [],
      "source": [
        "sequence_length = 90 \n",
        "\n",
        "\n",
        "def create_model(encoder, embed_dim, v_size_trg, latent_dim, num_heads):\n",
        "  encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  encoder_outputs = outputs.last_hidden_state\n",
        "\n",
        "  decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n",
        "  encoded_seq_inputs = tf.keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "  x = PositionalEmbedding(sequence_length, v_size_trg, embed_dim)(decoder_inputs)\n",
        "  x = layers.Dropout(dropout_rate)(x)\n",
        "  for i in range(8):\n",
        "    x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "\n",
        "  decoder_outputs = layers.Dense(v_size_trg, activation=\"softmax\")(x)\n",
        "\n",
        "  decoder = tf.keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "  decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "  transformer = tf.keras.Model(\n",
        "      [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        "  )\n",
        "  return transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ScU23iTwkf0g"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "\n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='./t5encoder-decoder.h5',\n",
        "    monitor=\"val_loss\",\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    mode=\"auto\",\n",
        "    save_freq=\"epoch\",\n",
        "    overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Search**"
      ],
      "metadata": {
        "id": "IOo1rxb2eQpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_src= \"google/t5-v1_1-small\"\n",
        "target_src = \"dbmdz/bert-base-italian-cased\" \n",
        "\n",
        "tokenizer_source1 = T5TokenizerFast.from_pretrained(source_src)\n",
        "tokenizer_target1 = BertTokenizer.from_pretrained(target_src)\n",
        "\n",
        "def metric(target, output):\n",
        "    import evaluate\n",
        "    predictions = [output]\n",
        "    references = [target]\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    results = bleu.compute(predictions=predictions, references=references)[\"bleu\"]\n",
        "    return results\n",
        "\n",
        "def clean(word):\n",
        "    clean_word = word.lower()\n",
        "    clean_word = clean_word.replace(\".\", \"\")\n",
        "    clean_word = clean_word.replace(\"\\\"\", \"\")\n",
        "    clean_word = clean_word.replace(\",\", \"\")\n",
        "    clean_word = clean_word.replace(\"â€™\", \"'\")\n",
        "    clean_word = clean_word.replace(\"[start]\", \"\")\n",
        "    clean_word = clean_word.replace(\"[end]\", \"\")\n",
        "    clean_word = clean_word.strip()\n",
        "    return clean_word\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence, tokenizer_source, tokenizer_target, transformer):\n",
        "    #tokenized_input_sentence=input_sentence\n",
        "    tokenized_input_sentence = tokenizer_source(input_sentence, return_tensors='tf', add_special_tokens=True, max_length = sequence_length, padding='max_length', truncation=True).data[\"input_ids\"]\n",
        "    decoded_sentence = \"[CLS]\"\n",
        "    list_tokens=[decoded_sentence]\n",
        "    for i in range(sequence_length):\n",
        "\n",
        "        decoded_sentence = tokenizer_target.convert_tokens_to_string(list_tokens)\n",
        "        tokenized_target_sentence = tokenizer_target(decoded_sentence, return_tensors='tf', add_special_tokens=False, max_length = sequence_length, padding='max_length').data['input_ids']\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = tokenizer_target.ids_to_tokens[sampled_token_index] #spa_index_lookup[sampled_token_index]\n",
        "        \n",
        "        #decoded_sentence += sampled_token\n",
        "       \n",
        "        if sampled_token == \"[SEP]\":\n",
        "          decoded_sentence = tokenizer_target.convert_tokens_to_string(list_tokens[1:])\n",
        "          break\n",
        "        list_tokens.append(sampled_token)\n",
        "    \n",
        "    return list_tokens, decoded_sentence\n",
        "\n",
        "def calculate_metric_on_testset(modello):\n",
        "  result = 0\n",
        "  count = 0\n",
        "  for item in test_pairs[:num_of_test]:\n",
        "      input_eng = item[0]   #eng\n",
        "      target = item[1]  #ita\n",
        "      # print(\"target: \", target)\n",
        "      output = decode_sequence(input_eng, tokenizer_source1, tokenizer_target1, modello)\n",
        "      item_result = metric(clean(target[1]),clean(output[1]))\n",
        "      result+=item_result\n",
        "      count+=1\n",
        "\n",
        "  final_result = result/count\n",
        "  return final_result\n"
      ],
      "metadata": {
        "id": "jcV443r1eAde"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KYfnhIUccTJk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b87ffc11daea40189522edf54b1c4a35",
            "c36a173514f34e9d86373beb2c1c360c",
            "2661fd510d8e4b5a8ed14c403bc1c044",
            "e6ca3f6a8ad141f1ba7c028ff580bad6",
            "e2d52aba45f748f7a735bb71561fb3b9",
            "0b4c929f7fd9457c96645620dad3e869",
            "8211f58b369b45198984c8528e047218",
            "c37cc02fec1d4f428ca56f66933be12f",
            "a55771c318e64780a887d43e4fecb387",
            "ca0da979f2324fc8b36b94318f2004a9",
            "89b2fe6f650e4535b94091a3f06b198d",
            "daa80018c38b4cceb82842908870f4f4",
            "d57dee7e1f0148bfb720b2526aec9716",
            "b7485af6d3a44579ae689042508c8c6b",
            "2d185eed7b6a4c14a4d44170ab60e38d",
            "e42f1a3760f54ff0ab12bce1c1a47a4c",
            "b8d0fd96f0b342838abb5bc02602b996",
            "089ac468deeb4bf5a06ed0a4d0c8d732",
            "b26f6afe76c24ae7b43aebf00ca9371a",
            "ad73f7ae8bcb47c8a9b6ad07f5c589ba",
            "14d5e93396584312a86b560c6f18391d",
            "7dc1cab5e5d04e48bc3addf69ef1459b",
            "b27b5e1c9ce14024b0bc986ef69de577",
            "95301916b2724fff954e152f2447aed1",
            "1de950458e954a428711b21368a62e75",
            "853600cda6c143abae3fa7f816a78d41",
            "d6f838b390a847ea8de2de8b2724c85d",
            "8b07e5ad35fa430a8829b31cc12c27f9",
            "d4d109fa794944f986c70ea52e41a5ea",
            "116cbf4e14824c87a1e3acdc7137ba7e",
            "a3bbbadef28f49e78ff5797995e81e04",
            "9f2466d12fde444e8f9a0af373c5c274",
            "ef5c25fe13cd4b7b8e9bfcff68c934a3"
          ]
        },
        "outputId": "41e566d6-9f3e-412b-dd0b-6bcd9bb7b5fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512   1024   8   128\n",
            "Epoch 1/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.4326 - accuracy: 0.2658\n",
            "Epoch 1: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 966s 333ms/step - loss: 0.4326 - accuracy: 0.2658 - val_loss: 0.3412 - val_accuracy: 0.3549\n",
            "Epoch 2/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.3704\n",
            "Epoch 2: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 704s 283ms/step - loss: 0.3084 - accuracy: 0.3704 - val_loss: 0.2517 - val_accuracy: 0.4286\n",
            "Epoch 3/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.4214\n",
            "Epoch 3: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 701s 281ms/step - loss: 0.2506 - accuracy: 0.4214 - val_loss: 0.2103 - val_accuracy: 0.4773\n",
            "Epoch 4/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.2216 - accuracy: 0.4519\n",
            "Epoch 4: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 702s 282ms/step - loss: 0.2216 - accuracy: 0.4519 - val_loss: 0.1894 - val_accuracy: 0.5010\n",
            "Epoch 5/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.4737\n",
            "Epoch 5: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 708s 284ms/step - loss: 0.2031 - accuracy: 0.4737 - val_loss: 0.1717 - val_accuracy: 0.5263\n",
            "Epoch 6/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1895 - accuracy: 0.4913\n",
            "Epoch 6: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 712s 286ms/step - loss: 0.1895 - accuracy: 0.4913 - val_loss: 0.1607 - val_accuracy: 0.5407\n",
            "Epoch 7/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.5049\n",
            "Epoch 7: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 713s 286ms/step - loss: 0.1789 - accuracy: 0.5049 - val_loss: 0.1518 - val_accuracy: 0.5535\n",
            "Epoch 8/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.5164\n",
            "Epoch 8: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 715s 287ms/step - loss: 0.1705 - accuracy: 0.5164 - val_loss: 0.1441 - val_accuracy: 0.5661\n",
            "Epoch 9/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.5272\n",
            "Epoch 9: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 713s 286ms/step - loss: 0.1632 - accuracy: 0.5272 - val_loss: 0.1388 - val_accuracy: 0.5733\n",
            "Epoch 10/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1574 - accuracy: 0.5357\n",
            "Epoch 10: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 716s 287ms/step - loss: 0.1574 - accuracy: 0.5357 - val_loss: 0.1339 - val_accuracy: 0.5828\n",
            "Epoch 11/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.5438\n",
            "Epoch 11: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 716s 288ms/step - loss: 0.1521 - accuracy: 0.5438 - val_loss: 0.1299 - val_accuracy: 0.5883\n",
            "Epoch 12/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1477 - accuracy: 0.5502\n",
            "Epoch 12: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 715s 287ms/step - loss: 0.1477 - accuracy: 0.5502 - val_loss: 0.1259 - val_accuracy: 0.5935\n",
            "Epoch 13/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1436 - accuracy: 0.5573\n",
            "Epoch 13: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 716s 287ms/step - loss: 0.1436 - accuracy: 0.5573 - val_loss: 0.1225 - val_accuracy: 0.6004\n",
            "Epoch 14/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.5608\n",
            "Epoch 14: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 718s 288ms/step - loss: 0.1410 - accuracy: 0.5608 - val_loss: 0.1193 - val_accuracy: 0.6046\n",
            "Epoch 15/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.5674\n",
            "Epoch 15: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 714s 287ms/step - loss: 0.1369 - accuracy: 0.5674 - val_loss: 0.1177 - val_accuracy: 0.6067\n",
            "Epoch 16/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1350 - accuracy: 0.5708\n",
            "Epoch 16: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 718s 288ms/step - loss: 0.1350 - accuracy: 0.5708 - val_loss: 0.1153 - val_accuracy: 0.6125\n",
            "Epoch 17/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.5753\n",
            "Epoch 17: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 720s 289ms/step - loss: 0.1318 - accuracy: 0.5753 - val_loss: 0.1143 - val_accuracy: 0.6124\n",
            "Epoch 18/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.5795\n",
            "Epoch 18: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 717s 288ms/step - loss: 0.1295 - accuracy: 0.5795 - val_loss: 0.1121 - val_accuracy: 0.6164\n",
            "Epoch 19/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.5837\n",
            "Epoch 19: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 716s 287ms/step - loss: 0.1272 - accuracy: 0.5837 - val_loss: 0.1104 - val_accuracy: 0.6191\n",
            "Epoch 20/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.5874\n",
            "Epoch 20: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 719s 289ms/step - loss: 0.1249 - accuracy: 0.5874 - val_loss: 0.1086 - val_accuracy: 0.6219\n",
            "Epoch 21/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.5912\n",
            "Epoch 21: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 713s 286ms/step - loss: 0.1230 - accuracy: 0.5912 - val_loss: 0.1077 - val_accuracy: 0.6257\n",
            "Epoch 22/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.5941\n",
            "Epoch 22: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 710s 285ms/step - loss: 0.1211 - accuracy: 0.5941 - val_loss: 0.1057 - val_accuracy: 0.6271\n",
            "Epoch 23/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1192 - accuracy: 0.5977\n",
            "Epoch 23: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 703s 282ms/step - loss: 0.1192 - accuracy: 0.5977 - val_loss: 0.1050 - val_accuracy: 0.6282\n",
            "Epoch 24/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1176 - accuracy: 0.6005\n",
            "Epoch 24: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 706s 283ms/step - loss: 0.1176 - accuracy: 0.6005 - val_loss: 0.1036 - val_accuracy: 0.6306\n",
            "Epoch 25/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.6037\n",
            "Epoch 25: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 704s 283ms/step - loss: 0.1159 - accuracy: 0.6037 - val_loss: 0.1028 - val_accuracy: 0.6326\n",
            "Epoch 26/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.6067\n",
            "Epoch 26: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 704s 283ms/step - loss: 0.1145 - accuracy: 0.6067 - val_loss: 0.1015 - val_accuracy: 0.6350\n",
            "Epoch 27/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.6097\n",
            "Epoch 27: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 709s 285ms/step - loss: 0.1130 - accuracy: 0.6097 - val_loss: 0.1004 - val_accuracy: 0.6357\n",
            "Epoch 28/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.6119\n",
            "Epoch 28: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 708s 284ms/step - loss: 0.1116 - accuracy: 0.6119 - val_loss: 0.1003 - val_accuracy: 0.6368\n",
            "Epoch 29/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.6150\n",
            "Epoch 29: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 704s 283ms/step - loss: 0.1102 - accuracy: 0.6150 - val_loss: 0.0999 - val_accuracy: 0.6392\n",
            "Epoch 30/30\n",
            "2491/2491 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.6177\n",
            "Epoch 30: saving model to ./t5encoder-decoder.h5\n",
            "2491/2491 [==============================] - 703s 282ms/step - loss: 0.1091 - accuracy: 0.6177 - val_loss: 0.0995 - val_accuracy: 0.6388\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.46k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b87ffc11daea40189522edf54b1c4a35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daa80018c38b4cceb82842908870f4f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b27b5e1c9ce14024b0bc986ef69de577"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "text_file = \"./dataset/ita.txt\"\n",
        "num_of_tests = 100\n",
        "\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, ita, _ = line.split(\"\\t\")\n",
        "    ita = \"[start] \" + ita + \" [end]\"\n",
        "    text_pairs.append((eng, ita))\n",
        "\n",
        "import random\n",
        "random.seed(39081)\n",
        "random.shuffle(text_pairs)\n",
        "\n",
        "num_train_samples = int(len(text_pairs) * 0.80)\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "test_pairs = text_pairs[num_train_samples:]\n",
        "\n",
        "RANDOM_SEARCH = False # if false, the model will be trained using the following hyperparameters:\n",
        "\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "EMBED_DIM = 512\n",
        "LATENT_DIM = 1024\n",
        "NUM_HEAD = 8\n",
        "\n",
        "random_search_trial = 4\n",
        "epochs = 30\n",
        "vocab_size = 15000\n",
        "sequence_length = 90\n",
        "max_decoded_sentence_length = 90\n",
        "num_of_test = 30 #Number of sentences in the test set used to calculate the metric\n",
        "\n",
        "models = []\n",
        "\n",
        "if not RANDOM_SEARCH:\n",
        "    random_search_trial = 1\n",
        "\n",
        "for _ in range(random_search_trial):\n",
        "\n",
        "  if not RANDOM_SEARCH:\n",
        "    batch_size = BATCH_SIZE\n",
        "  else:\n",
        "    batch_size = random.choice([8,16]) * strategy.num_replicas_in_sync\n",
        "\n",
        "  train_ds = make_dataset(tr_set, batch_size)\n",
        "  val_ds = make_dataset(val_set, batch_size)\n",
        "  test_ds = make_dataset(ts_set, batch_size)\n",
        "\n",
        "  if not RANDOM_SEARCH:\n",
        "    embed_dim = EMBED_DIM\n",
        "    latent_dim = LATENT_DIM\n",
        "    num_heads = NUM_HEAD\n",
        "  else:\n",
        "    embed_dim = random.choice([512])\n",
        "    latent_dim = random.choice([32,64,128,256,512])\n",
        "    num_heads = random.choice([6, 8])\n",
        "\n",
        "  with strategy.scope():\n",
        "    print(embed_dim, \" \", latent_dim, \" \", num_heads, \" \", batch_size)\n",
        "    transformer = create_model(encoder, embed_dim, v_size_trg, latent_dim, num_heads)\n",
        "    transformer.summary()\n",
        "    opt = tf.keras.optimizers.Adam()\n",
        "    transformer.compile(opt, loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    history = transformer.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=[model_checkpoint_callback])\n",
        "    metric_result = calculate_metric_on_testset(transformer)\n",
        "    lista = [str(batch_size), str(embed_dim), str(latent_dim), str(num_heads), \n",
        "            str(metric_result), str(epochs)]\n",
        "    models.append((transformer,metric_result, lista))\n",
        "                                                                                                                                                                                                  \n",
        "                                                                                                                                                                                                                                                                                                                                        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, best_result, best_params = models[np.argmax([y for x, y,z in models])]\n",
        "print(\"Best Result\", best_result)\n",
        "print(\"Best Parameters\", best_params)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  "
      ],
      "metadata": {
        "id": "tgDqDcyErdZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Translating**"
      ],
      "metadata": {
        "id": "FWbq2mXqKSQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_sentence, tokenizer_source, tokenizer_target, transformer):\n",
        "    #tokenized_input_sentence=input_sentence\n",
        "    tokenized_input_sentence = tokenizer_source(input_sentence, return_tensors='tf', add_special_tokens=True, max_length = sequence_length, padding='max_length', truncation=True).data[\"input_ids\"]\n",
        "    decoded_sentence = \"[CLS]\"\n",
        "    list_tokens=[decoded_sentence]\n",
        "    for i in range(sequence_length):\n",
        "\n",
        "        decoded_sentence = tokenizer_target.convert_tokens_to_string(list_tokens)\n",
        "        tokenized_target_sentence = tokenizer_target(decoded_sentence, return_tensors='tf', add_special_tokens=False, max_length = sequence_length, padding='max_length').data['input_ids']\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = tokenizer_target.ids_to_tokens[sampled_token_index] #spa_index_lookup[sampled_token_index]\n",
        "        \n",
        "        #decoded_sentence += sampled_token\n",
        "       \n",
        "        if sampled_token == \"[SEP]\":\n",
        "          decoded_sentence = tokenizer_target.convert_tokens_to_string(list_tokens[1:])\n",
        "          break\n",
        "        list_tokens.append(sampled_token)\n",
        "    \n",
        "    return list_tokens, decoded_sentence"
      ],
      "metadata": {
        "id": "4JzGKznrJNZM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "source_src= \"google/t5-v1_1-small\"\n",
        "target_src = \"dbmdz/bert-base-italian-cased\" \n",
        "\n",
        "tokenizer_source1 = T5TokenizerFast.from_pretrained(source_src)\n",
        "tokenizer_target1 = BertTokenizer.from_pretrained(target_src)\n",
        "\n",
        "def translation(sequence):\n",
        "  with strategy.scope():\n",
        "    tokens, translated = decode_sequence(sequence, tokenizer_source1, tokenizer_target1, peppino)\n",
        "  return translated"
      ],
      "metadata": {
        "id": "MAi5B6MZKGHY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_translate = \"I went to the restaurant with my friends\"\n",
        "\n",
        "print([to_translate, translation(to_translate)])"
      ],
      "metadata": {
        "id": "kWEpM86tKJAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_translate = \"I ate a lion with a friend and it was delicious\"\n",
        "\n",
        "print([to_translate, translation(to_translate)])"
      ],
      "metadata": {
        "id": "xuDaw-xfmYka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluator**"
      ],
      "metadata": {
        "id": "1n2L70i_8FNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"./dataset/ita.txt\"\n",
        "num_of_tests = 100\n",
        "\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, ita, _ = line.split(\"\\t\")\n",
        "    ita = \"[start] \" + ita + \" [end]\"\n",
        "    text_pairs.append((eng, ita))\n",
        "\n",
        "import random\n",
        "random.seed(39081)\n",
        "random.shuffle(text_pairs)\n",
        "\n",
        "num_train_samples = int(len(text_pairs) * 0.80)\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "test_pairs = text_pairs[num_train_samples:]\n",
        "\n",
        "def the_metric(target, output):\n",
        "    import evaluate\n",
        "    predictions = [output]\n",
        "    references = [target]\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    results = bleu.compute(predictions=predictions, references=references)[\"bleu\"]\n",
        "    return results\n",
        "\n",
        "def clean(word):\n",
        "    clean_word = word.lower()\n",
        "    clean_word = clean_word.replace(\".\", \"\")\n",
        "    clean_word = clean_word.replace(\"\\\"\", \"\")\n",
        "    clean_word = clean_word.replace(\",\", \"\")\n",
        "    clean_word = clean_word.replace(\"â€™\", \"'\")\n",
        "    clean_word = clean_word.replace(\"[start]\", \"\")\n",
        "    clean_word = clean_word.replace(\"[end]\", \"\")\n",
        "    clean_word = clean_word.strip()\n",
        "    return clean_word\n",
        "\n",
        "\n",
        "def evaluate_model(translate_function):\n",
        "    result = 0\n",
        "    count = 0\n",
        "    for i, item in enumerate(test_pairs[0:num_of_tests]):\n",
        "        input_eng = clean(item[0])  # eng\n",
        "        target = clean(item[1])  # ita\n",
        "        output = clean(translate_function(input_eng))\n",
        "        item_result = the_metric(target, output)\n",
        "        print(\"=================== TEST #\", i, \"===================\")\n",
        "        print(\"ðŸ‡®ðŸ‡¹ \", target)\n",
        "        print(\"ðŸ‡ºðŸ‡¸ \", input_eng)\n",
        "        print(\"Translated: \", output)\n",
        "        print(\"BLEU score: \", item_result)\n",
        "        result += item_result\n",
        "        count += 1\n",
        "\n",
        "    final_result = result / count\n",
        "    return final_result"
      ],
      "metadata": {
        "id": "DOOZNcEYJSQp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate_model(translation)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAwqdkWwUZ3x",
        "outputId": "5e2c2552-e3cb-4653-a3d3-dffdc8ff6a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================== TEST # 0 ===================\n",
            "ðŸ‡®ðŸ‡¹  dove andrÃ  in vacanza?\n",
            "ðŸ‡ºðŸ‡¸  where will you go for the vacation?\n",
            "Translated:  dove andre in vacanza vacanza\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 1 ===================\n",
            "ðŸ‡®ðŸ‡¹  parlare inglese non Ã¨ facile\n",
            "ðŸ‡ºðŸ‡¸  speaking english isn't easy\n",
            "Translated:  parlare inglese inglese inglese Ã¨ facile\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 2 ===================\n",
            "ðŸ‡®ðŸ‡¹  ho visto un aereo\n",
            "ðŸ‡ºðŸ‡¸  i saw a plane\n",
            "Translated:  ho visto visto un aereo\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 3 ===================\n",
            "ðŸ‡®ðŸ‡¹  lui Ã¨ occupato con i compiti a casa\n",
            "ðŸ‡ºðŸ‡¸  he's busy with his homework now\n",
            "Translated:  lui Ã¨ impegnato con i compiti compiti\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 4 ===================\n",
            "ðŸ‡®ðŸ‡¹  ha due anni in meno di lui\n",
            "ðŸ‡ºðŸ‡¸  she's two years younger than him\n",
            "Translated:  lei due anni in meno di lui lui\n",
            "BLEU score:  0.6803749333171202\n",
            "=================== TEST # 5 ===================\n",
            "ðŸ‡®ðŸ‡¹  andrÃ² a casa ora\n",
            "ðŸ‡ºðŸ‡¸  i'm going to go home now\n",
            "Translated:  andrÃ² a casa ora\n",
            "BLEU score:  1.0\n",
            "=================== TEST # 6 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom Ã¨ malato\n",
            "ðŸ‡ºðŸ‡¸  tom's sick\n",
            "Translated:  Ã¨ Ã¨ amma\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 7 ===================\n",
            "ðŸ‡®ðŸ‡¹  abbiamo fatto del volontariato\n",
            "ðŸ‡ºðŸ‡¸  we volunteered\n",
            "Translated:  ci siamo volontariato volontariato volontariato\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 8 ===================\n",
            "ðŸ‡®ðŸ‡¹  l'ho spento\n",
            "ðŸ‡ºðŸ‡¸  i turned it off\n",
            "Translated:  l ' spen\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 9 ===================\n",
            "ðŸ‡®ðŸ‡¹  lui non accetterebbe il denaro\n",
            "ðŸ‡ºðŸ‡¸  he would not take the money\n",
            "Translated:  [cls] non non accetterebbe i soldi  [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 10 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom non voleva giocare a golf col padre di mary\n",
            "ðŸ‡ºðŸ‡¸  tom didn't want to play golf with mary's father\n",
            "Translated:  non non voleva giocare golf golf il suo padre spezzato\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 11 ===================\n",
            "ðŸ‡®ðŸ‡¹  perchÃ© non suona piÃ¹ il banjo?\n",
            "ðŸ‡ºðŸ‡¸  why don't you play the banjo anymore?\n",
            "Translated:  perchÃ© non suona piÃ¹ il banjo ?\n",
            "BLEU score:  1.0\n",
            "=================== TEST # 12 ===================\n",
            "ðŸ‡®ðŸ‡¹  ho giÃ  sentito questa storia\n",
            "ðŸ‡ºðŸ‡¸  i've already heard this story\n",
            "Translated:  non ho sentito questa storia\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 13 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom ha iniziato a parlare\n",
            "ðŸ‡ºðŸ‡¸  tom began to speak\n",
            "Translated:  comin cominciato a parlare\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 14 ===================\n",
            "ðŸ‡®ðŸ‡¹  lei quanto spesso prende in prestito del denaro?\n",
            "ðŸ‡ºðŸ‡¸  how often do you borrow money?\n",
            "Translated:  quanto spesso prendi in prestito dei soldi ?\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 15 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom Ã¨ occupato ora quindi non riesce a parlare con lei\n",
            "ðŸ‡ºðŸ‡¸  tom is busy now so he can't talk with you\n",
            "Translated:  Ã¨ Ã¨ impegnato ora quindi non non riesce parlare parlare con\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 16 ===================\n",
            "ðŸ‡®ðŸ‡¹  se ne andÃ² dal ristorante senza pagare\n",
            "ðŸ‡ºðŸ‡¸  he left the restaurant without paying\n",
            "Translated:  [cls] lui andÃ² andÃ² ristorante senza senza pagare pagare [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 17 ===================\n",
            "ðŸ‡®ðŸ‡¹  io ero stupida\n",
            "ðŸ‡ºðŸ‡¸  i was stupid\n",
            "Translated:  ero stupido\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 18 ===================\n",
            "ðŸ‡®ðŸ‡¹  tornerÃ  fra un'ora\n",
            "ðŸ‡ºðŸ‡¸  she will return within an hour\n",
            "Translated:  lei ritorn ritorno tra un ' ora\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 19 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom e mary si stavano baciando quando sono entrata\n",
            "ðŸ‡ºðŸ‡¸  tom and mary were kissing each other when i walked in\n",
            "Translated:  essendo e e si si bacia bacia quando entrÃ² entrato\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 20 ===================\n",
            "ðŸ‡®ðŸ‡¹  penso che tom vincerÃ \n",
            "ðŸ‡ºðŸ‡¸  i think tom is going to win\n",
            "Translated:  io che che vince vincere\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 21 ===================\n",
            "ðŸ‡®ðŸ‡¹  tu potresti morire\n",
            "ðŸ‡ºðŸ‡¸  you could die\n",
            "Translated:  potreste morire\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 22 ===================\n",
            "ðŸ‡®ðŸ‡¹  pensavo che tom fosse in pensione\n",
            "ðŸ‡ºðŸ‡¸  i thought tom was retired\n",
            "Translated:  pensavo che che essere in pensione\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 23 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom Ã¨ fuggito per il rotto della cuffia\n",
            "ðŸ‡ºðŸ‡¸  tom escaped by the skin of his teeth\n",
            "Translated:  vennegg dalla dal rotto della denti denti\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 24 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom Ã¨ quello che ha rotto la finestra ieri\n",
            "ðŸ‡ºðŸ‡¸  tom is the one who broke the window yesterday\n",
            "Translated:  essendo Ã¨ quella che ha rotto la finestra ieri\n",
            "BLEU score:  0.6104735835807844\n",
            "=================== TEST # 25 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom disse di darle questo\n",
            "ðŸ‡ºðŸ‡¸  tom said to give you this\n",
            "Translated:  misi di darti questo\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 26 ===================\n",
            "ðŸ‡®ðŸ‡¹  sarei riuscita a farlo senza il suo aiuto\n",
            "ðŸ‡ºðŸ‡¸  i could've done that without your help\n",
            "Translated:  non sarei riuscito a farlo senza il tuo aiuto\n",
            "BLEU score:  0.3303164318013807\n",
            "=================== TEST # 27 ===================\n",
            "ðŸ‡®ðŸ‡¹  a cosa serve questo?\n",
            "ðŸ‡ºðŸ‡¸  what's this for?\n",
            "Translated:  a cosa serve questo ?\n",
            "BLEU score:  1.0\n",
            "=================== TEST # 28 ===================\n",
            "ðŸ‡®ðŸ‡¹  ha iniziato a piovere forte per tale ragione noi abbiamo suonato dentro\n",
            "ðŸ‡ºðŸ‡¸  it started raining hard because of that we played inside\n",
            "Translated:  haciÃ² a pioverevere per causa condizione abbiamo abbiamo dentro dentro\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 29 ===================\n",
            "ðŸ‡®ðŸ‡¹  io sono triste di vedervi partire\n",
            "ðŸ‡ºðŸ‡¸  i'm sad to see you go\n",
            "Translated:  sono Ã¨ triste di vedervi\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 30 ===================\n",
            "ðŸ‡®ðŸ‡¹  perchÃ© le piace fare questo?\n",
            "ðŸ‡ºðŸ‡¸  why do you like doing this?\n",
            "Translated:  perchÃ© le piace fare questo ?\n",
            "BLEU score:  1.0\n",
            "=================== TEST # 31 ===================\n",
            "ðŸ‡®ðŸ‡¹  sanno cos'Ã¨ successo\n",
            "ðŸ‡ºðŸ‡¸  they know what happened\n",
            "Translated:  sanno sanno ' ' Ã¨ successo\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 32 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom morÃ¬ con un coltello nella schiena\n",
            "ðŸ‡ºðŸ‡¸  tom died with a knife in his back\n",
            "Translated:  Ã¨ morto con un coltello in schiena schiena\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 33 ===================\n",
            "ðŸ‡®ðŸ‡¹  che grosso camion!\n",
            "ðŸ‡ºðŸ‡¸  what a big truck!\n",
            "Translated:  che grande camion !\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 34 ===================\n",
            "ðŸ‡®ðŸ‡¹  mio marito ha perso il suo impiego\n",
            "ðŸ‡ºðŸ‡¸  my husband lost his job\n",
            "Translated:  mio marito perse perso il lavoro\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 35 ===================\n",
            "ðŸ‡®ðŸ‡¹  io sono cosÃ¬ felice in questo momento\n",
            "ðŸ‡ºðŸ‡¸  i'm so happy right now\n",
            "Translated:  non sono cosÃ¬ felice in momento\n",
            "BLEU score:  0.4548019047027907\n",
            "=================== TEST # 36 ===================\n",
            "ðŸ‡®ðŸ‡¹  lei mi ha spiato\n",
            "ðŸ‡ºðŸ‡¸  you've been spying on me\n",
            "Translated:  mi avete spiata\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 37 ===================\n",
            "ðŸ‡®ðŸ‡¹  ciÃ² che pensa Ã¨ irrilevante\n",
            "ðŸ‡ºðŸ‡¸  what you think is irrelevant\n",
            "Translated:  quello che pensate Ã¨ irrilevante\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 38 ===================\n",
            "ðŸ‡®ðŸ‡¹  perchÃ© non sei pronto?\n",
            "ðŸ‡ºðŸ‡¸  why aren't you prepared?\n",
            "Translated:  perchÃ© non sei pronta\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 39 ===================\n",
            "ðŸ‡®ðŸ‡¹  non ho mai fatto il vostro nome\n",
            "ðŸ‡ºðŸ‡¸  i've never mentioned your name\n",
            "Translated:  non ho mai notato notato vostro nome\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 40 ===================\n",
            "ðŸ‡®ðŸ‡¹  che succede la settimana prossima?\n",
            "ðŸ‡ºðŸ‡¸  what's happening next week?\n",
            "Translated:  cosa succede la settimana prossima ?\n",
            "BLEU score:  0.7598356856515925\n",
            "=================== TEST # 41 ===================\n",
            "ðŸ‡®ðŸ‡¹  fai come se fossi a casa tua\n",
            "ðŸ‡ºðŸ‡¸  make yourself at home\n",
            "Translated:  [cls] fate in a casa casa casa casa  [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 42 ===================\n",
            "ðŸ‡®ðŸ‡¹  perchÃ© non sta ballando?\n",
            "ðŸ‡ºðŸ‡¸  why aren't you dancing?\n",
            "Translated:  perchÃ© non state ballando ?\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 43 ===================\n",
            "ðŸ‡®ðŸ‡¹  mettetela in freezer\n",
            "ðŸ‡ºðŸ‡¸  put it in the freezer\n",
            "Translated:  [cls] metti metta in freezer  [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 44 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom si svegliÃ² con un dolore nel fianco\n",
            "ðŸ‡ºðŸ‡¸  tom woke up with a pain in his side\n",
            "Translated:  si si svegliato con un dolore nel fianco fianco\n",
            "BLEU score:  0.46713797772820004\n",
            "=================== TEST # 45 ===================\n",
            "ðŸ‡®ðŸ‡¹  si dovrebbe prendere un altro paio di occhiali quando si va all'estero\n",
            "ðŸ‡ºðŸ‡¸  you should take another pair of glasses when you go abroad\n",
            "Translated:  dovrebbe prendere un altro paio di occhiali occhiali andate all ' estero\n",
            "BLEU score:  0.5156626918239822\n",
            "=================== TEST # 46 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom deve farlo presto\n",
            "ðŸ‡ºðŸ‡¸  tom needs to do that soon\n",
            "Translated:  devo farlo farlo presto\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 47 ===================\n",
            "ðŸ‡®ðŸ‡¹  il sistema funzionÃ²\n",
            "ðŸ‡ºðŸ‡¸  the system worked\n",
            "Translated:  il sistema ha funzionato\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 48 ===================\n",
            "ðŸ‡®ðŸ‡¹  ho portato il mio\n",
            "ðŸ‡ºðŸ‡¸  i brought mine\n",
            "Translated:  ho ho la mia\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 49 ===================\n",
            "ðŸ‡®ðŸ‡¹  io avevo ragione\n",
            "ðŸ‡ºðŸ‡¸  i was correct\n",
            "Translated:  non avevo ragione\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 50 ===================\n",
            "ðŸ‡®ðŸ‡¹  lui viene a trovarmi quasi ogni giorno\n",
            "ðŸ‡ºðŸ‡¸  he comes to see me nearly every day\n",
            "Translated:  lui a trovarmi a quasi giorno\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 51 ===================\n",
            "ðŸ‡®ðŸ‡¹  io voglio imparare a suonare lo xilofono\n",
            "ðŸ‡ºðŸ‡¸  i want to learn how to play the xylophone\n",
            "Translated:  non voglio imparare a suonare il xilofono\n",
            "BLEU score:  0.4347208719449915\n",
            "=================== TEST # 52 ===================\n",
            "ðŸ‡®ðŸ‡¹  sto iniziando a capire l'idea\n",
            "ðŸ‡ºðŸ‡¸  i'm starting to get the idea\n",
            "Translated:  sto iniziando cominciando a l ' ' idea\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 53 ===================\n",
            "ðŸ‡®ðŸ‡¹  penso che sia molto pericoloso\n",
            "ðŸ‡ºðŸ‡¸  i think it's very dangerous\n",
            "Translated:  non che sia molto pericoloso\n",
            "BLEU score:  0.668740304976422\n",
            "=================== TEST # 54 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom ha molte amiche?\n",
            "ðŸ‡ºðŸ‡¸  does tom have a lot of friends?\n",
            "Translated:  ha ha molti amiche ?\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 55 ===================\n",
            "ðŸ‡®ðŸ‡¹  dobbiamo partire immediatamente\n",
            "ðŸ‡ºðŸ‡¸  we must leave right away\n",
            "Translated:  dobbiamo partire immediatamente\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 56 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom ha perso tutto il suo denaro\n",
            "ðŸ‡ºðŸ‡¸  tom lost all of his money\n",
            "Translated:  pers perso tutti i suoi soldi\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 57 ===================\n",
            "ðŸ‡®ðŸ‡¹  il parco Ã¨ situato nel centro della cittÃ \n",
            "ðŸ‡ºðŸ‡¸  the park is located in the center of the city\n",
            "Translated:  il parco Ã¨ situato nel centro della cittÃ \n",
            "BLEU score:  1.0\n",
            "=================== TEST # 58 ===================\n",
            "ðŸ‡®ðŸ‡¹  vorrei passare un po' di tempo da solo con tom\n",
            "ðŸ‡ºðŸ‡¸  i'd like to have some time alone with tom\n",
            "Translated:  mi me avere un po ' ' tempo tempo tempo da andare\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 59 ===================\n",
            "ðŸ‡®ðŸ‡¹  io penso di capirti\n",
            "ðŸ‡ºðŸ‡¸  i think i understand you\n",
            "Translated:  penso di di capi\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 60 ===================\n",
            "ðŸ‡®ðŸ‡¹  da quando ti importa dell'etica?\n",
            "ðŸ‡ºðŸ‡¸  since when do you care about ethics?\n",
            "Translated:  da quando le importa dell ' etica ?\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 61 ===================\n",
            "ðŸ‡®ðŸ‡¹  avete appena detto il mio nome?\n",
            "ðŸ‡ºðŸ‡¸  did you just say my name?\n",
            "Translated:  ha appena detto il mio nome ?\n",
            "BLEU score:  0.8091067115702212\n",
            "=================== TEST # 62 ===================\n",
            "ðŸ‡®ðŸ‡¹  andate a lavarvi i denti!\n",
            "ðŸ‡ºðŸ‡¸  go brush your teeth\n",
            "Translated:  vai a lavarvi denti\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 63 ===================\n",
            "ðŸ‡®ðŸ‡¹  venda la sua roba\n",
            "ðŸ‡ºðŸ‡¸  sell your stuff\n",
            "Translated:  ven la tua roba\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 64 ===================\n",
            "ðŸ‡®ðŸ‡¹  noi possiamo essere ancora amiche\n",
            "ðŸ‡ºðŸ‡¸  we can still be friends\n",
            "Translated:  noi possiamo essere ancora\n",
            "BLEU score:  0.7788007830714049\n",
            "=================== TEST # 65 ===================\n",
            "ðŸ‡®ðŸ‡¹  penso che sarebbe stato affascinante\n",
            "ðŸ‡ºðŸ‡¸  i think that would've been fascinating\n",
            "Translated:  pensavo che sarebbe sarebbe stato\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 66 ===================\n",
            "ðŸ‡®ðŸ‡¹  questo Ã¨ del manzo davvero buono\n",
            "ðŸ‡ºðŸ‡¸  this is really good beef\n",
            "Translated:  [cls] questo Ã¨ carne man man buono buono  [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 67 ===================\n",
            "ðŸ‡®ðŸ‡¹  ridammi quel telefono\n",
            "ðŸ‡ºðŸ‡¸  give me that phone back\n",
            "Translated:  ridammi quel telefono\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 68 ===================\n",
            "ðŸ‡®ðŸ‡¹  sembri un po' pallida\n",
            "ðŸ‡ºðŸ‡¸  you look a little pale\n",
            "Translated:  sembri un po ' palli palli\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 69 ===================\n",
            "ðŸ‡®ðŸ‡¹  ho vissuto a boston per qualche anno\n",
            "ðŸ‡ºðŸ‡¸  i lived in boston for a few years\n",
            "Translated:  ho vissuto vissuto a per per qualche\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 70 ===================\n",
            "ðŸ‡®ðŸ‡¹  chi Ã¨ lui?\n",
            "ðŸ‡ºðŸ‡¸  who's he?\n",
            "Translated:  chi Ã¨ ?\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 71 ===================\n",
            "ðŸ‡®ðŸ‡¹  c'Ã¨ qualcosa che voglio dirle\n",
            "ðŸ‡ºðŸ‡¸  there's something that i want to tell you\n",
            "Translated:  c ' Ã¨ qualcosa che vuole voglio dirvi\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 72 ===================\n",
            "ðŸ‡®ðŸ‡¹  lo so che tom era il vostro migliore amico\n",
            "ðŸ‡ºðŸ‡¸  i know tom was your best friend\n",
            "Translated:  non che che era il il migliore amico\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 73 ===================\n",
            "ðŸ‡®ðŸ‡¹  non pensavo che tom avrebbe vinto\n",
            "ðŸ‡ºðŸ‡¸  i didn't think that tom would win\n",
            "Translated:  non non pensavo che avrebbe vinto\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 74 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom le ha mandato qualcosa\n",
            "ðŸ‡ºðŸ‡¸  tom sent you something\n",
            "Translated:  [cls] le manda mandato qualcosa  [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 75 ===================\n",
            "ðŸ‡®ðŸ‡¹  sono sicura\n",
            "ðŸ‡ºðŸ‡¸  i am sure\n",
            "Translated:  [cls] sono sono sicuro  [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 76 ===================\n",
            "ðŸ‡®ðŸ‡¹  non avevo intenzione di dire cosÃ¬ tanto\n",
            "ðŸ‡ºðŸ‡¸  i hadn't intended to say so much\n",
            "Translated:  non avevo avevo intenzione dire dire tanto\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 77 ===================\n",
            "ðŸ‡®ðŸ‡¹  lei ha un dizionario vero? posso usarlo?\n",
            "ðŸ‡ºðŸ‡¸  you have a dictionary don't you? can i use it?\n",
            "Translated:  hai un dizionario dizionario ? ? usarlo usarlo\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 78 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom ha molto dolore\n",
            "ðŸ‡ºðŸ‡¸  tom is in a lot of pain\n",
            "Translated:  lui ha molto dolore\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 79 ===================\n",
            "ðŸ‡®ðŸ‡¹  io non ho capito la domanda\n",
            "ðŸ‡ºðŸ‡¸  i didn't understand the question\n",
            "Translated:  non non capito capito domanda domanda\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 80 ===================\n",
            "ðŸ‡®ðŸ‡¹  mi colse di sorpresa\n",
            "ðŸ‡ºðŸ‡¸  it caught me by surprise\n",
            "Translated:  mi ha colto di sorpresa\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 81 ===================\n",
            "ðŸ‡®ðŸ‡¹  lavorammo tutto il giorno\n",
            "ðŸ‡ºðŸ‡¸  we worked all day\n",
            "Translated:  lavorammo tutto il giorno\n",
            "BLEU score:  1.0\n",
            "=================== TEST # 82 ===================\n",
            "ðŸ‡®ðŸ‡¹  sono molto impegnata ora\n",
            "ðŸ‡ºðŸ‡¸  i'm very busy now\n",
            "Translated:  sono sono molto impegnato ora\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 83 ===================\n",
            "ðŸ‡®ðŸ‡¹  inventÃ² una scusa\n",
            "ðŸ‡ºðŸ‡¸  he thought up an excuse\n",
            "Translated:  luiventÃ²tÃ² scusa scusa\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 84 ===================\n",
            "ðŸ‡®ðŸ‡¹  non avevamo niente di meglio da fare\n",
            "ðŸ‡ºðŸ‡¸  we had nothing better to do\n",
            "Translated:  non avevamo nulla di meglio da fare\n",
            "BLEU score:  0.488923022434901\n",
            "=================== TEST # 85 ===================\n",
            "ðŸ‡®ðŸ‡¹  non fuma\n",
            "ðŸ‡ºðŸ‡¸  she does not smoke\n",
            "Translated:  lei non\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 86 ===================\n",
            "ðŸ‡®ðŸ‡¹  sarÃ  a boston per natale?\n",
            "ðŸ‡ºðŸ‡¸  will you be in boston for christmas?\n",
            "Translated:  sarai anana per natale ?\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 87 ===================\n",
            "ðŸ‡®ðŸ‡¹  sii qui per le due e mezza\n",
            "ðŸ‡ºðŸ‡¸  be here by 2:30\n",
            "Translated:  sia qui per le due mezza\n",
            "BLEU score:  0.4548019047027907\n",
            "=================== TEST # 88 ===================\n",
            "ðŸ‡®ðŸ‡¹  pensa di poterlo riparare?\n",
            "ðŸ‡ºðŸ‡¸  do you think you can fix it?\n",
            "Translated:  pensa di poterlo riparare ?\n",
            "BLEU score:  1.0\n",
            "=================== TEST # 89 ===================\n",
            "ðŸ‡®ðŸ‡¹  che cosa stai facendo papÃ ?\n",
            "ðŸ‡ºðŸ‡¸  what are you doing dad?\n",
            "Translated:  cosa state facendo il papÃ  ?\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 90 ===================\n",
            "ðŸ‡®ðŸ‡¹  era semplicemente un disastro\n",
            "ðŸ‡ºðŸ‡¸  it was just a disaster\n",
            "Translated:  era solo un disastro\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 91 ===================\n",
            "ðŸ‡®ðŸ‡¹  siamo andati fino a kyoto\n",
            "ðŸ‡ºðŸ‡¸  we went as far as kyoto\n",
            "Translated:  noi siamo fino comen\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 92 ===================\n",
            "ðŸ‡®ðŸ‡¹  io penso che dovrebbe ascoltarmi\n",
            "ðŸ‡ºðŸ‡¸  i think you should listen to me\n",
            "Translated:  pensavo che che mi ascoltarmi\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 93 ===================\n",
            "ðŸ‡®ðŸ‡¹  tom lo sta tendendo\n",
            "ðŸ‡ºðŸ‡¸  tom is holding it\n",
            "Translated:  la la sta\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 94 ===================\n",
            "ðŸ‡®ðŸ‡¹  dica a tom che sto nuotando\n",
            "ðŸ‡ºðŸ‡¸  tell tom that i'm swimming\n",
            "Translated:  di di che che facendo nuotando\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 95 ===================\n",
            "ðŸ‡®ðŸ‡¹  devo studiare in continuo\n",
            "ðŸ‡ºðŸ‡¸  i have to study all the time\n",
            "Translated:  devo studiare studiare in\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 96 ===================\n",
            "ðŸ‡®ðŸ‡¹  stavo pranzando quando Ã¨ suonato il telefono\n",
            "ðŸ‡ºðŸ‡¸  i was eating lunch when the phone rang\n",
            "Translated:  stavo pranzando quando quando suonato suonato telefono\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 97 ===================\n",
            "ðŸ‡®ðŸ‡¹  studio l'inglese a casa\n",
            "ðŸ‡ºðŸ‡¸  i study english at home\n",
            "Translated:  studi inglese l inglese a\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 98 ===================\n",
            "ðŸ‡®ðŸ‡¹  ridalla\n",
            "ðŸ‡ºðŸ‡¸  give it back\n",
            "Translated:  ridallala\n",
            "BLEU score:  0.0\n",
            "=================== TEST # 99 ===================\n",
            "ðŸ‡®ðŸ‡¹  starÃ² qui per un paio di mesi\n",
            "ðŸ‡ºðŸ‡¸  i'm going to stay here for a couple of months\n",
            "Translated:  non starÃ² qui per paio paio di di\n",
            "BLEU score:  0.0\n",
            "0.14453696807306582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oDyzkQcGUmmH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "T5Encoder-Decoder from Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b87ffc11daea40189522edf54b1c4a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c36a173514f34e9d86373beb2c1c360c",
              "IPY_MODEL_2661fd510d8e4b5a8ed14c403bc1c044",
              "IPY_MODEL_e6ca3f6a8ad141f1ba7c028ff580bad6"
            ],
            "layout": "IPY_MODEL_e2d52aba45f748f7a735bb71561fb3b9"
          }
        },
        "c36a173514f34e9d86373beb2c1c360c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b4c929f7fd9457c96645620dad3e869",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8211f58b369b45198984c8528e047218",
            "value": "Downloading builder script: "
          }
        },
        "2661fd510d8e4b5a8ed14c403bc1c044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c37cc02fec1d4f428ca56f66933be12f",
            "max": 2463,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a55771c318e64780a887d43e4fecb387",
            "value": 2463
          }
        },
        "e6ca3f6a8ad141f1ba7c028ff580bad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca0da979f2324fc8b36b94318f2004a9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_89b2fe6f650e4535b94091a3f06b198d",
            "value": " 5.96k/? [00:00&lt;00:00, 81.6kB/s]"
          }
        },
        "e2d52aba45f748f7a735bb71561fb3b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b4c929f7fd9457c96645620dad3e869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8211f58b369b45198984c8528e047218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c37cc02fec1d4f428ca56f66933be12f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a55771c318e64780a887d43e4fecb387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca0da979f2324fc8b36b94318f2004a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89b2fe6f650e4535b94091a3f06b198d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daa80018c38b4cceb82842908870f4f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d57dee7e1f0148bfb720b2526aec9716",
              "IPY_MODEL_b7485af6d3a44579ae689042508c8c6b",
              "IPY_MODEL_2d185eed7b6a4c14a4d44170ab60e38d"
            ],
            "layout": "IPY_MODEL_e42f1a3760f54ff0ab12bce1c1a47a4c"
          }
        },
        "d57dee7e1f0148bfb720b2526aec9716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8d0fd96f0b342838abb5bc02602b996",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_089ac468deeb4bf5a06ed0a4d0c8d732",
            "value": "Downloading extra modules: "
          }
        },
        "b7485af6d3a44579ae689042508c8c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b26f6afe76c24ae7b43aebf00ca9371a",
            "max": 1554,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad73f7ae8bcb47c8a9b6ad07f5c589ba",
            "value": 1554
          }
        },
        "2d185eed7b6a4c14a4d44170ab60e38d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14d5e93396584312a86b560c6f18391d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7dc1cab5e5d04e48bc3addf69ef1459b",
            "value": " 4.07k/? [00:00&lt;00:00, 72.3kB/s]"
          }
        },
        "e42f1a3760f54ff0ab12bce1c1a47a4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8d0fd96f0b342838abb5bc02602b996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "089ac468deeb4bf5a06ed0a4d0c8d732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b26f6afe76c24ae7b43aebf00ca9371a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad73f7ae8bcb47c8a9b6ad07f5c589ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14d5e93396584312a86b560c6f18391d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dc1cab5e5d04e48bc3addf69ef1459b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b27b5e1c9ce14024b0bc986ef69de577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95301916b2724fff954e152f2447aed1",
              "IPY_MODEL_1de950458e954a428711b21368a62e75",
              "IPY_MODEL_853600cda6c143abae3fa7f816a78d41"
            ],
            "layout": "IPY_MODEL_d6f838b390a847ea8de2de8b2724c85d"
          }
        },
        "95301916b2724fff954e152f2447aed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b07e5ad35fa430a8829b31cc12c27f9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d4d109fa794944f986c70ea52e41a5ea",
            "value": "Downloading extra modules: "
          }
        },
        "1de950458e954a428711b21368a62e75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_116cbf4e14824c87a1e3acdc7137ba7e",
            "max": 1289,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3bbbadef28f49e78ff5797995e81e04",
            "value": 1289
          }
        },
        "853600cda6c143abae3fa7f816a78d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f2466d12fde444e8f9a0af373c5c274",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ef5c25fe13cd4b7b8e9bfcff68c934a3",
            "value": " 3.34k/? [00:00&lt;00:00, 54.0kB/s]"
          }
        },
        "d6f838b390a847ea8de2de8b2724c85d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b07e5ad35fa430a8829b31cc12c27f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d109fa794944f986c70ea52e41a5ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "116cbf4e14824c87a1e3acdc7137ba7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3bbbadef28f49e78ff5797995e81e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f2466d12fde444e8f9a0af373c5c274": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef5c25fe13cd4b7b8e9bfcff68c934a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}